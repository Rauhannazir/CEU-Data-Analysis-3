---
title: "Find best hotels with machine learning"
subtitle: "DA3 Assignment 3"
author: "Viktória Mészáros"
date: "14/02/2021"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, include=F}
rm(list=ls())

# Load packages
library(dplyr)
library(tidyverse)
library(stargazer)
library(lspline)
library(caret)
library(segmented) 
library(kableExtra)
library(rpart)
library(rpart.plot)
library(rattle)

dir <- "C:/Users/MViki/Documents/CEU/Winter_semester/DA_3/Classes/Assignments/CEU-Data-Analysis-3/Assignment_3/"

#location folders
data_in <- paste0(dir,"/Data/Clean/")
data_out <- data_in
output <- paste0(dir,"Output/")

# load Vienna
data <- read_csv("https://raw.githubusercontent.com/Viki-Meszaros/CEU-Data-Analysis-3/main/Assignment_3/Data/Clean/hotels-vienna.csv")

options(digits = 4)

```


## Introduction
The aim of this analysis is to find the best deals for hotels in Vienna. This is built on the "Finding a Good Deal among Hotels with Multiple Regressions" case study in the 10th chapter. For this I will use different machine learning models to predict the price of the hotels based on the different features they have. After this I will search for the places where the actual price is the lowest compare to the predicted (calculating most negative residuals). I will outline the 5 best hotels based on the best models and compare them to the findings of the original case study. 

## Data preparation
We start the analysis with the *hotelbookingdata* which contains data about 23 thousand different hotels in 33 unique countries collected between 2017 November and 2018 June. As we are only interested in the hotels in Vienna we filter down our data set to this city. In Vienna there were 516 observations, but a lot of them were in the table multiple times due to different points in time. I wanted one observation for each hotel, so for this I filter the data set down to only a given year and month to get comparable values. I chose 2017 November weekdays, the same what we used in the case study as I wanted to get an output that is comparable. After dropping duplicated rows my data set decreased to 428 observations.

## Explanatory data analysis
### Filtering
I started explanatory data analysis with looking at different possible values as well as extremes. I wanted to find the best deal among hotels so I had to filter out other accommodation types. After checking actual city I realized that some hotels are not actually located in Vienna. I deleted those. I only kept observations that had 3, 3.5 or 4 stars and that were at maximum 8 miles away from the city center. I also excluded hotels that had extreme prices. With all these I had a final 207 hotels from which I wanted to find the best deal. 

```{r, echo=F, message=F, warning=F, out.width="50%"}

ggplot(data =  data, aes (x = distance)) +
  geom_histogram(color = "gray25", fill = "cyan4", alpha= 0.8 ,binwidth = 0.5)+
  labs(x = "Distance to city center (miles)", y = "Frequency") +
  geom_segment(aes(x = 8.2, y = 0, xend = 8.2, yend = 90), color = "deeppink4", alpha = 0.6, size=1) +
  annotate("text", x = 11, y = 29, label = "Too far out", size=2)+
  annotate("rect",xmin=8.2, xmax=14, ymin=0, ymax=90, fill="deeppink4", alpha=0.1)+
  theme_bw()

data <- data %>% 
  filter(accommodation_type=="Hotel") %>%
  filter(city_actual=="Vienna") %>%
  filter(stars>=3 & stars<=4) %>% filter(!is.na(stars)) 

ggplot(data =  data, aes (x = price)) +
  geom_histogram(color = "gray25", fill = "cyan4",alpha = 0.8)+
  labs(x = "Price (US dollars)", y = "Frequency") +
  geom_segment(aes(x = 600, y = 0, xend = 600, yend = 70), color = "deeppink4", alpha = 0.6, size=1) +
  annotate("text", x = 800, y = 30, label = "Too expensive", size=2)+
  annotate("rect",xmin=600, xmax=1100, ymin=0, ymax=70, fill="deeppink4", alpha=0.1)+
  theme_bw()

data <- data %>% 
  filter(price<=600) 
```

### Missing values
I only had missing values for TripAdvisors rating and thus rating count fro 3 hotels. I decided to use imputation in these cases. I substituted 4 as the TripAdvisor rating for these observations as that was the rounded mean and median for all the others. For TripAdvisors rating count I imputed the mean of all other rating counts. I also added flags for the imputed variables.

```{r, include=F}
data <- data %>%
  mutate(
    ratingta =  ifelse(is.na(ratingta), 4, ratingta),
    flag_ratingta=ifelse(is.na(ratingta),1, 0),
    ratingta_count =  ifelse(is.na(ratingta_count), mean(ratingta_count, na.rm = T), ratingta_count),
    flag_ratingta_count=ifelse(is.na(ratingta_count),1, 0))

# Stars: binary indicators - for regression in book's case study
data$star35 = ifelse(data$stars==3.5, 1, 0)
data$star4 = ifelse(data$stars==4, 1, 0)

# Transform neighborhood to factor
data$neighbourhood_f <- as.factor(data$neighbourhood)

# Offer to factor
data$offer_cat_f<- as.factor(data$offer_cat)

# Number of start to factor - 3 different values
data$stars_f <- as.factor(data$stars)

# Ratingta to factor - 5 different values
data$ratingta_f <- as.factor(data$ratingta)
```


### Potential functional forms
The outcome variable in the  analysis will be **Price**. Let's look at the distribution of prices. We can see that it is skewed with a long right tail. In these cases it is advised to do a logarithmic transformation. On the second graph you can see that taking the log of price transforms its distribution to closer to normal. Based on this we are going to predict **lnprice** instead of price.

```{r, echo=F, message=F, warning=F, out.width="50%"}
### PRICE - skewed with long right tail
ggplot(data =  data, aes (x = price, y = ..density..)) +
  geom_histogram(color = "gray25", fill = "cyan4",alpha = 0.8)+
  labs(x = "Price (US dollars)", y = "Frequency") +
  theme_bw()

### LNPRICE - closer  to normal distribution
ggplot(data =  data, aes (x = log(price), y = ..density..)) +
  geom_histogram(color = "gray25", fill = "cyan4",alpha = 0.8)+
  labs(x = "Price (US dollars)", y = "Frequency") +
  theme_bw()

# Take the log for price
data$lnprice <- log(data$price)

```

For distance from the city center and rating I included piecewise linear spline as the pattern is not even close to simple linear, but it is different for different intervals. For this I searched for the best cuts with an algorithm. FBased on this, for distance there will be three knots at 1.1, 2.9 and 4.3 while for rating there will be two at 3.2 and 3.8. 


```{r, echo=F, message=F, warning=F, out.width="50%"}
### Distance 
ggplot(data = data, aes(x = distance, y = log(price))) +
  geom_point(color = "grey25") + 
  geom_smooth(method = "loess", color = "cyan4")+
  labs(x = "Distance to city center (miles)",y = "ln(price, US dollars)")+
  theme_bw()

ggplot(data = data, aes(x = rating, y = log(price))) +
  geom_point(color = "grey25") + 
  geom_smooth(method = "loess", color = "cyan4")+
  labs(x = "Rating",y = "ln(price, US dollars)")+
  theme_bw()
```


## Modelling

### Set up
The aim of the analysis was to find the best deals among hotels. For this I built 4 different types of models to predict prices and then find the most underpriced hotels. The outcome variable for all the models will be **lnprice**. Fot the feature variables I grouped them into three separate categories:

* **basic variables**    --    *lspline(distance) + lspline(rating) + stars*
* **ratings variables**  --    *rating count + TripAdvisor rating + TripAdvisor rating count + flags for missing values*
* **extra variables**    --    *scarce romm dummy + offer dummy + offer category + neighborhood*

I built three simple ols models with different levels of complexity, one LASSO model containing all variable, one CART model and two random forest models one with manual tuning grid and one with auto tuning. For these instead of the piecewise linear spline in distance and in rating I used simply the distance and the rating. The following table helps to understand what features are included in what models. I used a 10 fold cross validation during training all the models. 

```{r, include=F}
reg0 <- lm(lnprice ~ lspline(distance, c(1,4)) + lspline(rating, 3.5) + star35 + star4, data=data)

# Variables
basic <- c("lspline(distance, c(1.1, 2.9, 4.3))",  "lspline(rating, c(3.2, 3.8))", "stars_f")
ratings <- c("rating_count", "ratingta_f", "ratingta_count", "flag_ratingta", "flag_ratingta_count")
extra <- c("scarce_room", "offer", "offer_cat_f", "neighbourhood_f")

pred_1 <- c(basic)
pred_2 <- c(basic, ratings)
pred_3 <- c(basic, ratings, extra)
pred_rf <- c("distance", "rating", "stars_f", ratings, extra)

preds <- list("pred_1" = pred_1, "pred_2" = pred_2, "pred_3" = pred_3)

train_control <- trainControl(method = "cv", number = 10, verboseIter = FALSE)

set.seed(2021)
system.time({
  simple_reg <- train(
    formula(lnprice ~ lspline(distance, c(1,4)) + lspline(rating, 3.5) + star35 + star4),
    data = data,
    method = "lm",
    trControl = train_control
  )
})
```


```{r, echo=F, message=F, warning=F}
models <- data.frame(row.names = c("OLS 1", "OLS 2", "OLS 3", "LASSO", "CART", "Random forest"))
models$Variables[1] <- "basic"
models$Variables[2] <- "basic + ratings"
models$Variables[3] <- "basic + ratings + extra"
models$Variables[4] <- "basic + ratings + extra"
models$Variables[5] <- "distance + rating + starts + ratings + extra"
models$Variables[6] <- "distance + rating + starts + ratings + extra"

models %>% 
  kbl() %>% 
  kable_classic(full_width = F, html_font = "Cambria")
```


```{r, include=F}
#################################
#           OLS                 #
#################################

ols_model_coeffs <- list()
ols_models <- list()

for (model in names(preds)) {
  
  features <- preds[[model]]
  
  set.seed(2021)
  system.time({
    ols_model <- train(
      formula(paste0("lnprice ~", paste0(features, collapse = " + "))),
      data = data,
      method = "lm",
      trControl = train_control
    )
  })
  
  ols_model_coeffs[[model]] <-  ols_model$finalModel$coefficients
  ols_models[[model]]<- ols_model
}


#################################
#           LASSO               #
#################################

set.seed(2021)
system.time({
  lasso_model <- train(
    formula(paste0("lnprice ~", paste0(pred_3, collapse = " + "))),
    data = data,
    method = "glmnet",
    preProcess = c("center", "scale"),
    tuneGrid =  expand.grid("alpha" = 1, "lambda" = seq(0.01, 10, by = 0.01)),
    trControl = train_control
  )
})


lasso_coeffs <- coef(
  lasso_model$finalModel,
  lasso_model$bestTune$lambda) %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column(var = "variable") %>%
  rename(lasso_coefficient = `1`)  # the column has a name "1", to be renamed

lasso_coeffs_nz <- lasso_coeffs[!lasso_coeffs$lasso_coefficient == 0,]


# Evaluate model. CV error:
lasso_cv_rmse <- lasso_model$results %>%
  filter(lambda == lasso_model$bestTune$lambda) %>%
  dplyr::select(RMSE)



#################################
#            CART               #
#################################

set.seed(2021)
system.time({
  cart_model <- train(
    formula(paste0("lnprice ~", paste0(pred_rf, collapse = " + "))),
    data = data,
    method = "rpart",
    tuneLength = 10,
    trControl = train_control
  )
})


#################################
#       Random forest           #
#################################

# set tuning 
tune_grid <- expand.grid(
  .mtry = c(1:15),
  .splitrule = "variance",
  .min.node.size = c(1:15)
)

set.seed(2021)
system.time({
  rf_model <- train(
    formula(paste0("lnprice ~", paste0(pred_rf, collapse = " + "))),
    data = data,
    method = "ranger",
    tuneGrid = tune_grid,
    importance = "impurity",
    trControl = train_control
  )
})


# auto tuning first
set.seed(2021)
system.time({
  rf_model_auto <- train(
    formula(paste0("lnprice ~", paste0(pred_rf, collapse = " + "))),
    data = data,
    method = "ranger",
    importance = "impurity",
    trControl = train_control
  )
})


```

### Comparison
To compare the predicting performance of the different models I use the cross validated RMSE averaged through all the fols and the R squared for the models. The simple reg refers to the regression in the case study including only distance, rating and stars as feature variables. Among the OLS models the best is the third one containing all the variables. Surprisingly the most simple one with only the basic variables outperform the second more complex model. The LASSO model turned out to be the best out of all with a RMSE of **0.21** and an R squared of **62.4%** meaning that the features in the model are responsible for 67.9% of the variation in the price. A simple tree built with CART performed the worst while the random forest with a hand set tune grid became the second best model. Both random forest and the LASSO outperforms the basic regression used durong the case study to find the best deals. To find the best deals I will use both the LASSO and the random forest.


```{r, echo=F, message=F, warning=F}
#####################################################################
# SUMMARY
#####################################################################

final_models <-
  list("Simple reg" = simple_reg,
       "OLS 1" = ols_models$pred_1,
       "OLS 2" = ols_models$pred_2,
       "OLS 3" = ols_models$pred_3,
       "LASSO" = lasso_model,
       "CART" = cart_model,
       "Random forest"= rf_model,
       "Random forest auto" = rf_model_auto
  )

results <- resamples(final_models) %>% summary()

result_RMSE <- imap(final_models, ~{
  mean(results$values[[paste0(.y,"~RMSE")]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("CV RMSE" = ".")


result_Rsqr <- imap(final_models, ~{
  mean(results$values[[paste0(.y,"~Rsquared")]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("CV R squared" = ".")


comparison <- cbind(result_RMSE, result_Rsqr)

comparison%>% 
  kbl() %>% 
  kable_classic(full_width = F, html_font = "Cambria")

```

## Best deals
I decided to look at the 5 best deals which has the smallest residual values both for the LASSO an the random forest models. With predicting the log of price when looking at the most negative residuals to find the best deals we are looking at highest percentage difference. Here we are not looking at the best hotels where that has the highest USD difference between the actual price and the predicted but the percentage difference (for example here it is better deal when the actual price is 50 compared to a predicted price of 55, than when the actual is 200 compared to 215). The table below shows the best five deals for the simple regression in the book, the LASSO , the cart and the random forest models I built.
Let's compare the best 5 hotels I got wit different models and what we found during the initial case study. It is interesting to see that there are several hotels that came out to be the best deal for more models. 

* The best hotel based on the book's regression, lasso and the random forest is 21912, and it is also in the top 5 for the cart
* 22344 is second for both lasso and random forest models, and third for the cart 
* 22080 is in the best five for lasso as well
* 22184 is also listed for cart and random forest
* 21975 is not considered a best deal for other models 


```{r, echo=F, message=F, warning=F}
# Simple reg residuals
data$lnprice_hat <- predict(reg0)
data$lnprice_resid <- data$lnprice - data$lnprice_hat
data$bestdeals <- ifelse(data$lnprice_resid %in% tail(sort(data$lnprice_resid, decreasing=TRUE),5),TRUE,FALSE)

reg <- data %>%
  select(hotel_id, lnprice_resid) %>%
  arrange(lnprice_resid) %>%
  .[1:5,] %>%
  as.data.frame()

# LASSO residuals
data$lnprice_pred_lasso <- predict(lasso_model, newdata = data)
data$lasso_resid <- data$lnprice - data$lnprice_pred_lasso
data$bestdeals_lasso <- ifelse(data$lasso_resid %in% tail(sort(data$lasso_resid, decreasing=TRUE),5),TRUE,FALSE)

StDev_lasso <- sd(data$lasso_resid)
data$price_pred_lasso <- exp(data$lnprice_pred_lasso) * exp((StDev_lasso^2)/2)

lasso <- data %>%
  select(hotel_id, lasso_resid) %>%
  arrange(lasso_resid) %>%
  .[1:5,] %>%
  as.data.frame() 

data$lasso_resid_2 <- data$price - data$price_pred_lasso

# CART residuals
data$lnprice_pred_cart <- predict(cart_model, newdata = data)
data$cart_resid <- data$lnprice - data$lnprice_pred_cart
data$bestdeals_cart <- ifelse(data$cart_resid %in% tail(sort(data$cart_resid, decreasing=TRUE),5),TRUE,FALSE)

StDev_cart <- sd(data$cart_resid)
data$price_pred_cart <- exp(data$lnprice_pred_cart) * exp((StDev_cart^2)/2)

cart <- data %>%
  select(hotel_id, cart_resid) %>%
  arrange(cart_resid) %>%
  .[1:5,] %>%
  as.data.frame() 

data$cart_resid_2 <- data$price - data$price_pred_cart

# Random forests residuals
data$lnprice_pred_rf <- predict(rf_model, newdata = data)
data$rf_resid <- data$lnprice - data$lnprice_pred_rf 
data$bestdeals_rf <- ifelse(data$rf_resid %in% tail(sort(data$rf_resid, decreasing=TRUE),5),TRUE,FALSE)

StDev_rf <- sd(data$rf_resid)
data$price_pred_rf <- exp(data$lnprice_pred_rf) * exp((StDev_rf^2)/2)

rf <- data %>%
  select(hotel_id, rf_resid) %>%
  arrange(rf_resid) %>%
  .[1:5,] %>%
  as.data.frame() 

best_deals <- cbind(reg, lasso, cart, rf)

names(best_deals) <- c("Reg in book", "reg_resid", "Lasso", "laso_resid", "Cart", "cart_resid", "Random Forest", "rf_resid")

best_deals %>% 
  kbl() %>% 
  kable_classic(full_width = F, html_font = "Cambria")



```

### Visuals
These visuals give a nice representation of the best deals. These are the dots that are furthest down from the 45°line. These are really similar to what we saw for the regression in the book, except for the cart model. There we see only some predicted prices, those are the values for the final nodes in the tree. You can see the whole tree in the appendix.

```{r, echo=F, message=F, warning=F, out.width="50%"}
ggplot(data = data, aes(x = lnprice_pred_lasso, y = lnprice)) +
  geom_point(aes(color=bestdeals_lasso,shape=bestdeals_lasso), size = 1.5, fill="deeppink4", alpha = 0.8,  show.legend=F, na.rm = TRUE) + 
  geom_segment(aes(x = 3.8, y = 3.8, xend = 6, yend =6), size=0.8, color="cyan4", linetype=2) +
  labs(title = "Fit of LASSO model",x = "ln(predicted price, US dollars) ",y = "ln(price, US dollars)")+
  scale_colour_manual(name='',values=c("grey25",'black')) +
  scale_shape_manual(name='',values=c(16,21)) +
  geom_segment(aes(x = 4.8, y = 3.9, xend = 4.68, yend = 4.05), arrow = arrow(length = unit(0.1, "cm")))+
  annotate("text", x = 4.93, y = 3.9, label = "Best deal", size=2.5)+
  theme_bw()

ggplot(data = data, aes(x = lnprice_pred_cart, y = lnprice)) +
  geom_point(aes(color=bestdeals_cart,shape=bestdeals_cart), size = 1.5, fill="deeppink4", alpha = 0.8,  show.legend=F, na.rm = TRUE) + 
  geom_segment(aes(x = 3.8, y = 3.8, xend = 6, yend =6), size=0.8, color="cyan4", linetype=2) +
  labs(title = "Fit of CART model",x = "ln(predicted price, US dollars) ",y = "ln(price, US dollars)")+
  scale_colour_manual(name='',values=c("grey25",'black')) +
  scale_shape_manual(name='',values=c(16,21)) +
  theme_bw()

ggplot(data = data, aes(x = lnprice_pred_rf, y = lnprice)) +
  geom_point(aes(color=bestdeals_rf,shape=bestdeals_rf), size = 1.5, fill="deeppink4", alpha = 0.8,  show.legend=F, na.rm = TRUE) + 
  geom_segment(aes(x = 3.8, y = 3.8, xend = 6, yend =6), size=0.8, color="cyan4", linetype=2) +
  labs(title = "Fit of Random Forest model",x = "ln(predicted price, US dollars) ",y = "ln(price, US dollars)")+
  scale_colour_manual(name='',values=c("grey25",'black')) +
  scale_shape_manual(name='',values=c(16,21)) +
  theme_bw()

```

## Summary
From this project we can make an interesting conclusion. We can build really complex machine learning models to predict prices more precisely an find the best deals among hotels, but in the end the results we get are really similar to what we can find with a really simple linear regression with only three features.  Sometimes it is better to stick with easy, simple models and methods and don't bother with complex things as in the end it does not give a significant improvement. In our case we managed to increase out R squared from 0.55 to 0.58 with the random forest and even further to 0.62 with the lasso model.   

## Appendix

### How price is depending on the different features
The following charts were part of the explanatory data analysis. Here I looked at how price depends on each variable. 

#### Number of stars

```{r, echo=F, message=F, warning=F, out.width="40%"}
### Number of stars
ggplot(data = data, aes(x = stars_f, y = price)) +
  stat_boxplot(aes(group = stars_f), geom = "errorbar", width = 0.3,
               color = c("green4","cyan4", "deeppink4"), size = 0.5, na.rm=T)+
  geom_boxplot(aes(group = stars_f),
               color = c("green4","cyan4", "deeppink4"), fill = c("green4","cyan4", "deeppink4"),
               size = 0.5, width = 0.6, alpha = 0.3, na.rm=T, outlier.shape = NA) +
  scale_y_continuous(expand = c(0.01,0.01),limits = c(0,300), breaks = seq(0,300,100)) +
  labs(y = "Price (US dollars)")+
  theme_bw()

```

#### TipAdvisor rating
```{r, echo=F, message=F, warning=F, out.width="40%"}
### TripAdvisor rating
ggplot(data = data, aes(x = ratingta_f, y = price)) +
  stat_boxplot(aes(group = ratingta_f), geom = "errorbar", width = 0.3,
               color = c("green4","cyan4", "purple4", "deeppink4", "darkorange2"), size = 0.5, na.rm=T)+
  geom_boxplot(aes(group = ratingta_f),
               color = c("green4","cyan4", "purple4", "deeppink4", "darkorange2"), 
               fill = c("green4","cyan4", "purple4", "deeppink4", "darkorange2"),
               size = 0.5, width = 0.6, alpha = 0.3, na.rm=T, outlier.shape = NA) +
  scale_y_continuous(expand = c(0.01,0.01),limits = c(0,300), breaks = seq(0,300,100)) +
  labs(y = "Price (US dollars)")+
  theme_bw()

```

#### Scarce room
```{r, echo=F, message=F, warning=F, out.width="40%"}
### Scarce room - does not really have an effect
ggplot(data = data, aes(x = scarce_room, y = price)) +
  stat_boxplot(aes(group = scarce_room), geom = "errorbar", width = 0.3,
               color = c("cyan4", "deeppink4"), size = 0.5, na.rm=T)+
  geom_boxplot(aes(group = scarce_room),
               color = c("cyan4", "deeppink4"), 
               fill = c("cyan4", "deeppink4"),
               size = 0.5, width = 0.6, alpha = 0.3, na.rm=T, outlier.shape = NA) +
  scale_y_continuous(expand = c(0.01,0.01),limits = c(0,300), breaks = seq(0,300,100)) +
  labs(y = "Price (US dollars)")+
  theme_bw()

```

#### Offer
```{r, echo=F, message=F, warning=F, out.width="40%"}
### Offer
ggplot(data = data, aes(x = offer, y = price)) +
  stat_boxplot(aes(group = offer), geom = "errorbar", width = 0.3,
               color = c("cyan4", "deeppink4"), size = 0.5, na.rm=T)+
  geom_boxplot(aes(group = offer),
               color = c("cyan4", "deeppink4"), 
               fill = c("cyan4", "deeppink4"),
               size = 0.5, width = 0.6, alpha = 0.3, na.rm=T, outlier.shape = NA) +
  scale_y_continuous(expand = c(0.01,0.01),limits = c(0,300), breaks = seq(0,300,100)) +
  labs(y = "Price (US dollars)")+
  theme_bw()
```

#### Offer category
```{r, echo=F, message=F, warning=F, out.width="40%"}
### Offer category
ggplot(data = data, aes(x = offer_cat_f, y = price)) +
  stat_boxplot(aes(group = offer_cat_f), geom = "errorbar", width = 0.3,
               color = c("green4","cyan4", "deeppink4", "darkorange2"), size = 0.5, na.rm=T)+
  geom_boxplot(aes(group = offer_cat_f),
               color = c("green4","cyan4", "deeppink4", "darkorange2"), 
               fill = c("green4","cyan4", "deeppink4", "darkorange2"),
               size = 0.5, width = 0.6, alpha = 0.3, na.rm=T, outlier.shape = NA) +
  scale_y_continuous(expand = c(0.01,0.01),limits = c(0,300), breaks = seq(0,300,100)) +
  labs(y = "Price (US dollars)")+
  theme_bw()
```

#### Neighborhood
```{r, echo=F, message=F, warning=F, out.width="60%"}
### Neighborhood
ggplot(data = data, aes(x = neighbourhood_f, y = price)) +
  stat_boxplot(aes(group = neighbourhood_f), geom = "errorbar", width = 0.3,
               color = "cyan4", size = 0.5, na.rm=T)+
  geom_boxplot(aes(group = neighbourhood_f),
               color = "cyan4", 
               fill = "cyan4",
               size = 0.5, width = 0.6, alpha = 0.3, na.rm=T, outlier.shape = NA) +
  scale_y_continuous(expand = c(0.01,0.01),limits = c(0,300), breaks = seq(0,300,100)) +
  labs(y = "Price (US dollars)")+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

```

### CART model graph
```{r, echo=F, message=F, warning=F, out.width="60%"}
fancyRpartPlot(cart_model$finalModel, sub = "")
```


```{r, echo=F, message=F, warning=F, out.width="50%", fig.align='center'}

```













